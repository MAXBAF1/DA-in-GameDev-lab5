# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #5 выполнил:
- Лепинских Максим Игоревич
- РИ-220943 

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

## Цель работы
### Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найдите внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.

В файле RollerAgent.cs внутри метода OnActionReceived был найден такой код:
```C#
float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

if(distanceToTarget < 1.42f)
{
  SetReward(1.0f);
  EndEpisode();
}
```
Здесь коэффициентом корреляции является значение **1.42**, это связь между Агентом и Целью.
- При уменьшении этого коэффициента, тем точнее, сложнее и дольше будет обучение, так как он определяет, насколько близко шар должен находиться к цели, чтобы получить вознаграждение (вызвалось SetReward(1.0f)).
- При увеличении же, наоборот обучение будет происходить быстрее и легче, но с меньшей точностью, так как шар будет не слишком близко к цели когда получает награду.

![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/a74b4307-d4d5-4eb0-abbd-c7d09b476d9b)

  
## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
Процесс обучения будет отображен в виде графиков на TensorBoard, чтобы визуализировать, как изменение определенного параметра влияет на динамику обучения. Для каждой попытки обучения будет проведено 100 тысяч шагов. Я буду брать не самые пороговые значения, но близкие к ним, чтобы одновременно не уходить в крайности и наблюдать разницу.
#### Economic.yaml

#### 1. extrinsic -> strength - Коэффициент, на который умножается награда.

strength: 0.1

![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/6fcb6f34-3635-41eb-86a1-a9a42a877b72)
![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/f73af025-dc7f-4aa6-af83-50cf8e6df7b4)


strength: 10

![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/0241ed32-61c3-4bbe-bb61-9492828fee03)
![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/2762c461-1c88-43d5-b4f1-87a96599f6f9)

По графикам видно, что strength не сильно влияет на Cumulative Reward, однако его влияние можно увидеть на графиках ошибок, при strength: 0.1 он довольно ровный и имеет нисходящий тренд, но вот при значении 10 его начинает колбасить и ошибок становится много.

#### 2. play_against_latest_model_ratio определяет, насколько вероятно, что агент будет действовать в противоположность предыдущей модели. Низкие значения означают, что агент будет предпочитать похожие действия, что может затруднить процесс обучения. С другой стороны, высокие значения могут вызвать нестабильность в обучающей среде, но позволяют агенту адаптироваться к более сложным ситуациям, что в итоге может привести к формированию более эффективной стратегии. Поэтому важно подбирать значение этого параметра в золотой середине. Он изменяется в пределах от 0.0 до 1.0.

play_against_latest_model_ratio: 0.2:

![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/0c6a4788-146b-4219-8f5d-f5c24b457a68)
![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/32cc70d7-b9b5-4a39-bd91-1ffb577dbb7e)

play_against_latest_model_ratio: 0.8:

![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/cbc1f620-981b-4327-a6a5-6f9956bd2d5b)
![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/d948a671-2f9b-4243-a01d-cd1109cd0e53)

Мы видим, что при значении play_against_latest_model_ratio: 0.8 график вознаграждения ведёт себя более гладко, по сравнению со значением 0.2, однако они приходят в конце концов примерно на один уровень. Сильную разницу видно на графиках ошибок, при совершении большого количества новых действий, ошибок больше примерно в 10 раз по сравнению, с тем, когда модель была консервативной. Это означает, что при низких значениях модель обучается медленее и это происходит рывками, график ведёт себя нестабильно, наоборот вторая модель каждый раз совершала всё новое и новое для себя, много раз ошиблась, но надёжно и плавно пришла к цели. 

#### 3. time_horizon - Сколько шагов опыта нужно набрать, прежде чем добавлять агента в буфер опыта. Типичный диапазон: 32-2048

time_horizon: 64

![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/b8d99331-4255-4223-a842-b215bbc18d0b)
![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/228a4455-6dfa-4ce6-a259-fdd2d5072965)

time_horizon: 1024

![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/4a331b24-fb71-43da-8efd-9c7a0efd2994)
![image](https://github.com/MAXBAF1/DA-in-GameDev-lab5/assets/63009846/4a6b74c5-caec-44a7-bdf3-91c8824aa14c)

При маленьком значении time_horizon награждение довольно стабильное, но вот при большом, оно начинается вообще с отрицательного и медленно-медленно идёт к заветной единице. Это обуславливается тем, что при втором случае агенту нужно сделать много лишних действий, которые мешают ему получить награду.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения? 

Я думаю ML-Agent, у которого есть путь, используется в играх, где не всегда маршрут от одной точки для другой одинаковый, например, на нём может возникнуть припятствие (блоки, обломки или даже сам игрок). Обычному алгоритму, в котором прописан чёткий путь по координатам просто упёрся бы в него, а мл-агент умно обойдёт. В примеры таких игр можно привести более менее приличные гонки по типу Need for Speed или любую игру с качественными NPC.

Использовать ML-агента можно в случае где мир может изменяться под воздействием игрока, тогда программная реализация решения достаточно нетривиальна и не необходима. 

## Выводы

Я познакомился с программными средствами для создания системы машинного обучения и ее интеграции в Unity, а так же научился использовать TensorBoard для построения графиков.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
